{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "dpath = Path(\"data/YearPredictionMSD.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "with dpath.open(\"r\") as infile:\n",
    "    for line in infile.readlines():\n",
    "        line_split = line.replace(\"\\n\", \"\").split(\",\")\n",
    "        year = int(line_split[0])\n",
    "        inputs = [*map(lambda x: float(x), line_split[1:])]\n",
    "        records.append([year] + inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"year\"] + [f\"feature_{i}\" for i in range(1, df.shape[-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_81</th>\n",
       "      <th>feature_82</th>\n",
       "      <th>feature_83</th>\n",
       "      <th>feature_84</th>\n",
       "      <th>feature_85</th>\n",
       "      <th>feature_86</th>\n",
       "      <th>feature_87</th>\n",
       "      <th>feature_88</th>\n",
       "      <th>feature_89</th>\n",
       "      <th>feature_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>49.94357</td>\n",
       "      <td>21.47114</td>\n",
       "      <td>73.07750</td>\n",
       "      <td>8.74861</td>\n",
       "      <td>-17.40628</td>\n",
       "      <td>-13.09905</td>\n",
       "      <td>-25.01202</td>\n",
       "      <td>-12.23257</td>\n",
       "      <td>7.83089</td>\n",
       "      <td>...</td>\n",
       "      <td>13.01620</td>\n",
       "      <td>-54.40548</td>\n",
       "      <td>58.99367</td>\n",
       "      <td>15.37344</td>\n",
       "      <td>1.11144</td>\n",
       "      <td>-23.08793</td>\n",
       "      <td>68.40795</td>\n",
       "      <td>-1.82223</td>\n",
       "      <td>-27.46348</td>\n",
       "      <td>2.26327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>48.73215</td>\n",
       "      <td>18.42930</td>\n",
       "      <td>70.32679</td>\n",
       "      <td>12.94636</td>\n",
       "      <td>-10.32437</td>\n",
       "      <td>-24.83777</td>\n",
       "      <td>8.76630</td>\n",
       "      <td>-0.92019</td>\n",
       "      <td>18.76548</td>\n",
       "      <td>...</td>\n",
       "      <td>5.66812</td>\n",
       "      <td>-19.68073</td>\n",
       "      <td>33.04964</td>\n",
       "      <td>42.87836</td>\n",
       "      <td>-9.90378</td>\n",
       "      <td>-32.22788</td>\n",
       "      <td>70.49388</td>\n",
       "      <td>12.04941</td>\n",
       "      <td>58.43453</td>\n",
       "      <td>26.92061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>50.95714</td>\n",
       "      <td>31.85602</td>\n",
       "      <td>55.81851</td>\n",
       "      <td>13.41693</td>\n",
       "      <td>-6.57898</td>\n",
       "      <td>-18.54940</td>\n",
       "      <td>-3.27872</td>\n",
       "      <td>-2.35035</td>\n",
       "      <td>16.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>3.03800</td>\n",
       "      <td>26.05866</td>\n",
       "      <td>-50.92779</td>\n",
       "      <td>10.93792</td>\n",
       "      <td>-0.07568</td>\n",
       "      <td>43.20130</td>\n",
       "      <td>-115.00698</td>\n",
       "      <td>-0.05859</td>\n",
       "      <td>39.67068</td>\n",
       "      <td>-0.66345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>48.24750</td>\n",
       "      <td>-1.89837</td>\n",
       "      <td>36.29772</td>\n",
       "      <td>2.58776</td>\n",
       "      <td>0.97170</td>\n",
       "      <td>-26.21683</td>\n",
       "      <td>5.05097</td>\n",
       "      <td>-10.34124</td>\n",
       "      <td>3.55005</td>\n",
       "      <td>...</td>\n",
       "      <td>34.57337</td>\n",
       "      <td>-171.70734</td>\n",
       "      <td>-16.96705</td>\n",
       "      <td>-46.67617</td>\n",
       "      <td>-12.51516</td>\n",
       "      <td>82.58061</td>\n",
       "      <td>-72.08993</td>\n",
       "      <td>9.90558</td>\n",
       "      <td>199.62971</td>\n",
       "      <td>18.85382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>50.97020</td>\n",
       "      <td>42.20998</td>\n",
       "      <td>67.09964</td>\n",
       "      <td>8.46791</td>\n",
       "      <td>-15.85279</td>\n",
       "      <td>-16.81409</td>\n",
       "      <td>-12.48207</td>\n",
       "      <td>-9.37636</td>\n",
       "      <td>12.63699</td>\n",
       "      <td>...</td>\n",
       "      <td>9.92661</td>\n",
       "      <td>-55.95724</td>\n",
       "      <td>64.92712</td>\n",
       "      <td>-17.72522</td>\n",
       "      <td>-1.49237</td>\n",
       "      <td>-7.50035</td>\n",
       "      <td>51.76631</td>\n",
       "      <td>7.88713</td>\n",
       "      <td>55.66926</td>\n",
       "      <td>28.74903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  2001   49.94357   21.47114   73.07750    8.74861  -17.40628  -13.09905   \n",
       "1  2001   48.73215   18.42930   70.32679   12.94636  -10.32437  -24.83777   \n",
       "2  2001   50.95714   31.85602   55.81851   13.41693   -6.57898  -18.54940   \n",
       "3  2001   48.24750   -1.89837   36.29772    2.58776    0.97170  -26.21683   \n",
       "4  2001   50.97020   42.20998   67.09964    8.46791  -15.85279  -16.81409   \n",
       "\n",
       "   feature_7  feature_8  feature_9  ...  feature_81  feature_82  feature_83  \\\n",
       "0  -25.01202  -12.23257    7.83089  ...    13.01620   -54.40548    58.99367   \n",
       "1    8.76630   -0.92019   18.76548  ...     5.66812   -19.68073    33.04964   \n",
       "2   -3.27872   -2.35035   16.07017  ...     3.03800    26.05866   -50.92779   \n",
       "3    5.05097  -10.34124    3.55005  ...    34.57337  -171.70734   -16.96705   \n",
       "4  -12.48207   -9.37636   12.63699  ...     9.92661   -55.95724    64.92712   \n",
       "\n",
       "   feature_84  feature_85  feature_86  feature_87  feature_88  feature_89  \\\n",
       "0    15.37344     1.11144   -23.08793    68.40795    -1.82223   -27.46348   \n",
       "1    42.87836    -9.90378   -32.22788    70.49388    12.04941    58.43453   \n",
       "2    10.93792    -0.07568    43.20130  -115.00698    -0.05859    39.67068   \n",
       "3   -46.67617   -12.51516    82.58061   -72.08993     9.90558   199.62971   \n",
       "4   -17.72522    -1.49237    -7.50035    51.76631     7.88713    55.66926   \n",
       "\n",
       "   feature_90  \n",
       "0     2.26327  \n",
       "1    26.92061  \n",
       "2    -0.66345  \n",
       "3    18.85382  \n",
       "4    28.74903  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(515345, 91)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As per instructions https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD\n",
    "train_samples = 463715\n",
    "train = df.iloc[:train_samples,:]\n",
    "test = df.iloc[train_samples:, :]\n",
    "\n",
    "train_y = train.year.values\n",
    "test_y = test.year.values\n",
    "train_x = train.iloc[:, 1:].values\n",
    "test_x = test.iloc[:, 1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transform outcome variable (left-skewed)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "max_out_train = np.max(train_y)\n",
    "train_y_reflected = np.log(1 + max_out_train - train_y)\n",
    "test_y_reflected = np.log(1 + max_out_train - test_y)\n",
    "\n",
    "sns.displot(train_y_reflected)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from piven.regressors import build_keras_piven\n",
    "from piven.loss import piven_loss\n",
    "from piven.metrics import mpiw, picp\n",
    "from piven.transformers import PivenTransformedTargetRegressor\n",
    "from piven.wrappers import PivenModelWrapper\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "# Make build function for the model wrapper\n",
    "def piven_model(input_dim, dense_units, dropout_rate):\n",
    "    model = build_keras_piven(input_dim=input_dim,\n",
    "                             dense_units=dense_units,\n",
    "                             dropout_rate=dropout_rate,\n",
    "                             activation=\"relu\",\n",
    "                             bias_init_low=-3,\n",
    "                             bias_init_high=3)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
    "                  loss=piven_loss(15.0, 160.0, 0.05),\n",
    "                  metrics=[picp, mpiw])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model using the MLP that comes with piven.py\n",
    "model = PivenModelWrapper(build_fn=piven_model, \n",
    "                          input_dim=train_x.shape[-1], \n",
    "                          dense_units=(64,32), \n",
    "                          dropout_rate=(0.0,0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model in pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocess\", StandardScaler()),\n",
    "    (\"model\", model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to transformed target regressor\n",
    "model_ttr = PivenTransformedTargetRegressor(\n",
    "    regressor=pipeline,\n",
    "    transformer=StandardScaler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5797/5797 [==============================] - 6s 962us/step - loss: 9.0029 - picp: 0.8872 - mpiw: 4.7301 - val_loss: 6.1304 - val_picp: 0.8965 - val_mpiw: 3.7906\n",
      "Epoch 2/15\n",
      "5797/5797 [==============================] - 5s 942us/step - loss: 5.3478 - picp: 0.9140 - mpiw: 3.8339 - val_loss: 5.4624 - val_picp: 0.9102 - val_mpiw: 3.6206\n",
      "Epoch 3/15\n",
      "5797/5797 [==============================] - 5s 917us/step - loss: 4.8871 - picp: 0.9165 - mpiw: 3.5963 - val_loss: 5.1506 - val_picp: 0.9142 - val_mpiw: 3.4773\n",
      "Epoch 4/15\n",
      "5797/5797 [==============================] - 5s 896us/step - loss: 4.6420 - picp: 0.9175 - mpiw: 3.4364 - val_loss: 4.9175 - val_picp: 0.9235 - val_mpiw: 3.4521\n",
      "Epoch 5/15\n",
      "5797/5797 [==============================] - 5s 896us/step - loss: 4.5005 - picp: 0.9186 - mpiw: 3.3496 - val_loss: 4.7935 - val_picp: 0.9289 - val_mpiw: 3.4434\n",
      "Epoch 6/15\n",
      "5797/5797 [==============================] - 5s 907us/step - loss: 4.3764 - picp: 0.9198 - mpiw: 3.2769 - val_loss: 4.8130 - val_picp: 0.9171 - val_mpiw: 3.2416\n",
      "Epoch 7/15\n",
      "5797/5797 [==============================] - 5s 923us/step - loss: 4.2846 - picp: 0.9200 - mpiw: 3.2072 - val_loss: 4.8060 - val_picp: 0.9089 - val_mpiw: 3.1112\n",
      "Epoch 8/15\n",
      "5797/5797 [==============================] - 5s 933us/step - loss: 4.2196 - picp: 0.9195 - mpiw: 3.1608 - val_loss: 4.6381 - val_picp: 0.9204 - val_mpiw: 3.1831\n",
      "Epoch 9/15\n",
      "5797/5797 [==============================] - 5s 943us/step - loss: 4.1472 - picp: 0.9216 - mpiw: 3.1341 - val_loss: 4.6674 - val_picp: 0.9112 - val_mpiw: 3.0551\n",
      "Epoch 10/15\n",
      "5797/5797 [==============================] - 5s 944us/step - loss: 4.1084 - picp: 0.9215 - mpiw: 3.1024 - val_loss: 4.5388 - val_picp: 0.9213 - val_mpiw: 3.1093\n",
      "Epoch 11/15\n",
      "5797/5797 [==============================] - 5s 942us/step - loss: 4.0580 - picp: 0.9221 - mpiw: 3.0749 - val_loss: 4.5781 - val_picp: 0.9126 - val_mpiw: 3.0044\n",
      "Epoch 12/15\n",
      "5797/5797 [==============================] - 5s 944us/step - loss: 4.0247 - picp: 0.9208 - mpiw: 3.0326 - val_loss: 4.5823 - val_picp: 0.9099 - val_mpiw: 2.9589\n",
      "Epoch 13/15\n",
      "5797/5797 [==============================] - 5s 942us/step - loss: 3.9889 - picp: 0.9217 - mpiw: 3.0174 - val_loss: 4.4604 - val_picp: 0.9183 - val_mpiw: 3.0009\n",
      "Epoch 14/15\n",
      "5797/5797 [==============================] - 5s 948us/step - loss: 3.9702 - picp: 0.9186 - mpiw: 2.9721 - val_loss: 4.5428 - val_picp: 0.9105 - val_mpiw: 2.9173\n",
      "Epoch 15/15\n",
      "5797/5797 [==============================] - 5s 938us/step - loss: 3.9307 - picp: 0.9211 - mpiw: 2.9642 - val_loss: 4.4683 - val_picp: 0.9176 - val_mpiw: 2.9576\n"
     ]
    }
   ],
   "source": [
    "# Fit\n",
    "import numpy as np\n",
    "h = model_ttr.fit(train_x, train_y_reflected, model__epochs=15, model__validation_split=0.2, model__batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred, y_pi_low, y_pi_high = model_ttr.predict(test_x, return_prediction_intervals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back-transform the predictions and PIs\n",
    "def back_transform(x, max_train):\n",
    "    xexp = np.exp(x)\n",
    "    return max_train + 1 - xexp\n",
    "ypred = back_transform(ypred, max_out_train)\n",
    "# Need to reverse the bounds\n",
    "y_pi_low_t = back_transform(y_pi_high, max_out_train)\n",
    "y_pi_high_t = back_transform(y_pi_low, max_out_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9161533991865195"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from piven.metrics import coverage, pi_distance\n",
    "coverage(test_y, y_pi_low_t, y_pi_high_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.921993"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_distance(y_pi_low_t, y_pi_high_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame({\n",
    "    \"true\":test_y,\n",
    "    \"pred\":ypred,\n",
    "    \"pi_low\":y_pi_low_t,\n",
    "    \"pi_high\":y_pi_high_t\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pivenregressor/lib/python3.7/site-packages/pandas/core/computation/expressions.py:204: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  f\"evaluating in Python space because the {repr(op_str)} \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46070460704607047"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers = df_res[df_res.true < 1960]\n",
    "coverage(outliers.true, outliers.pi_low, outliers.pi_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.89465"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_distance(outliers.pi_low, outliers.pi_high)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
